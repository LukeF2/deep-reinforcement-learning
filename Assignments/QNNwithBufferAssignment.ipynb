{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install gymnasium"],"metadata":{"id":"KJUp6QBRY-Qd","executionInfo":{"status":"ok","timestamp":1739471712189,"user_tz":480,"elapsed":3474,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"7f0173bd-b027-48b4-d2e7-c4b5a999d196","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"]}]},{"cell_type":"code","source":["from time import sleep\n","import numpy as np\n","from IPython.display import clear_output\n","import gymnasium as gym\n","from gymnasium.envs.registration import register\n","import torch\n","from torch import nn\n"],"metadata":{"id":"K_GLg7lbayhm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Give colab access to your google drive:\n","from google.colab import drive\n","drive.mount('/gdrive')"],"metadata":{"id":"7PWN1PkGe66q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739471985286,"user_tz":480,"elapsed":20100,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"6a38c50c-e8b9-447b-9743-e9d321d4b26c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["#Change current directory to folder with MiniPacMan\n","%cd /gdrive/MyDrive/CSCI181V/"],"metadata":{"id":"1SCX1d90YjOg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739471985286,"user_tz":480,"elapsed":3,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"7d3086f3-b264-416a-fc6d-0b1528f8f8df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/CSCI181V\n"]}]},{"cell_type":"code","source":["#Import MiniPacMan environment class definition\n","from MiniPacManGymV2 import MiniPacManEnv"],"metadata":{"id":"GCa5TYdVWL2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Register MiniPacMan in your gymnasium environments\n","register(\n","    id=\"MiniPacMan-v0\",\n","    entry_point=MiniPacManEnv,  # Update with your actual module path\n","    max_episode_steps=20          # You can also set a default here\n",")"],"metadata":{"id":"TcY1Q97RRy6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create a MiniPacMan gymnasium environment\n","env = gym.make(\"MiniPacMan-v0\", render_mode=\"human\", frozen_ghost=False)"],"metadata":{"id":"k7hwnC7Ob9VJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class QNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear1 = nn.Linear(6 * 6, 128)  # Increase hidden units\n","        self.activation1 = nn.ReLU()\n","        self.linear2 = nn.Linear(128, 64)\n","        self.activation2 = nn.ReLU()\n","        self.linear3 = nn.Linear(64, 4)  # 4 actions\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.linear1(x)\n","        x = self.activation1(x)\n","        x = self.linear2(x)\n","        x = self.activation2(x)\n","        x = self.linear3(x)\n","        return x\n"],"metadata":{"id":"Y6irumLQsc1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_network = QNetwork()\n","target_network = QNetwork()\n","target_network.load_state_dict(pred_network.state_dict())\n","Q_optimizer = torch.optim.Adam(pred_network.parameters(), lr=0.0001) #feel free to change this\n","loss_fn = nn.MSELoss()"],"metadata":{"id":"6-TgasGh9Q11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ReplayBuffer:\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.buffer = []\n","\n","    def push(self, state, action, reward, next_state, done):\n","        if len(self.buffer) >= self.capacity:\n","            self.buffer.pop(0)\n","        self.buffer.append((state, action, reward, next_state, done))\n","\n","    def sample(self, batch_size):\n","        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n","        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n","        return torch.stack(states), actions, torch.tensor(rewards), torch.stack(next_states), torch.tensor(dones)"],"metadata":{"id":"8D31lBLpRUC0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def update_target_network(pred_network, target_network):\n","    target_network.load_state_dict(pred_network.state_dict())"],"metadata":{"id":"2RbTUnDSb96i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#set hyperparams -- play with any of these!\n","gamma=0.99\n","buffer_size=1000\n","batch_size=64\n","num_episodes=10000\n","epsilon_min = 0.1\n","epsilon_decay = 0.9997\n","target_update_interval = 100\n","RB=ReplayBuffer(buffer_size) #initialize Replay Buffer\n","epsilon=1 #initialize epsilon\n","\n","for e in range(num_episodes):\n","  new_obs,info=env.reset()\n","  new_obs=torch.tensor(new_obs,dtype=torch.float32)\n","\n","  done=False\n","  truncated=False\n","  steps=0\n","\n","  while not done and not truncated: #Loop for one episode\n","    obs=new_obs\n","    state_tensor = obs.unsqueeze(0)\n","\n","    #choose action\n","    t=np.random.random()\n","    if t>epsilon:\n","      with torch.no_grad():\n","          q_values = pred_network(state_tensor)\n","          action = torch.argmax(q_values, dim=1).item()\n","    else:\n","      action=torch.randint(4,(1,)).item()\n","\n","    #take a step:\n","    new_obs,reward, done, truncated, info=env.step(action)\n","    new_obs=torch.tensor(new_obs,dtype=torch.float32)\n","    RB.push(obs,action,reward,new_obs,done)\n","    steps+=1\n","\n","    if len(RB.buffer)>=batch_size:\n","      states, actions, rewards, next_states, dones=RB.sample(batch_size)\n","\n","      #Define predictions, targets, loss here\n","      actions = torch.tensor(actions, dtype=torch.long)  # Ensure it's integer type\n","      current_q = pred_network(states)[range(batch_size), actions]\n","\n","\n","      # Compute the next state's Q-values.\n","      with torch.no_grad():\n","        next_q = target_network(next_states)\n","        max_next_q, _ = next_q.max(dim=1)  # Get max Q-value for next state\n","      target_q = rewards + gamma * max_next_q * (1 - dones.float())\n","\n","      #loss\n","      loss = loss_fn(current_q, target_q)\n","      Q_optimizer.zero_grad()\n","      loss.backward()\n","      Q_optimizer.step()\n","\n","  #reduce episilon if its not too low:\n","  epsilon = max(epsilon_min, epsilon * epsilon_decay)\n","\n","  #periodic reporting:\n","  if e>0 and e%100==0:\n","    print(f'episode: {e}, steps: {steps}, epislon: {epsilon},win: {reward==10}')\n","\n","  if e % target_update_interval == 0:\n","        update_target_network(pred_network, target_network)\n","        print(f'Target network updated at episode {e}')\n"],"metadata":{"id":"0fe-YvvwKpAZ","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739472533166,"user_tz":480,"elapsed":273648,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"bd87bf42-2f8c-4c1f-fb9e-b7d1280a91a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Target network updated at episode 0\n","episode: 100, steps: 1, epislon: 0.9701500333301386,win: False\n","Target network updated at episode 100\n","episode: 200, steps: 5, epislon: 0.9414735292292377,win: False\n","Target network updated at episode 200\n","episode: 300, steps: 4, epislon: 0.913644669161937,win: False\n","Target network updated at episode 300\n","episode: 400, steps: 1, epislon: 0.8866383977586842,win: False\n","Target network updated at episode 400\n","episode: 500, steps: 1, epislon: 0.8604304002574455,win: False\n","Target network updated at episode 500\n","episode: 600, steps: 1, epislon: 0.8349970806122093,win: False\n","Target network updated at episode 600\n","episode: 700, steps: 2, epislon: 0.8103155402485782,win: False\n","Target network updated at episode 700\n","episode: 800, steps: 5, epislon: 0.786363557447322,win: False\n","Target network updated at episode 800\n","episode: 900, steps: 1, epislon: 0.7631195673373274,win: False\n","Target network updated at episode 900\n","episode: 1000, steps: 5, epislon: 0.7405626424799333,win: False\n","Target network updated at episode 1000\n","episode: 1100, steps: 2, epislon: 0.7186724740271707,win: False\n","Target network updated at episode 1100\n","episode: 1200, steps: 2, epislon: 0.6974293534369442,win: False\n","Target network updated at episode 1200\n","episode: 1300, steps: 2, epislon: 0.6768141547286871,win: False\n","Target network updated at episode 1300\n","episode: 1400, steps: 3, epislon: 0.6568083172635245,win: False\n","Target network updated at episode 1400\n","episode: 1500, steps: 8, epislon: 0.6373938290334299,win: False\n","Target network updated at episode 1500\n","episode: 1600, steps: 3, epislon: 0.6185532104443396,win: False\n","Target network updated at episode 1600\n","episode: 1700, steps: 1, epislon: 0.6002694985786144,win: False\n","Target network updated at episode 1700\n","episode: 1800, steps: 15, epislon: 0.582526231922685,win: False\n","Target network updated at episode 1800\n","episode: 1900, steps: 6, epislon: 0.5653074355461368,win: False\n","Target network updated at episode 1900\n","episode: 2000, steps: 11, epislon: 0.5485976067188758,win: False\n","Target network updated at episode 2000\n","episode: 2100, steps: 2, epislon: 0.5323817009534375,win: False\n","Target network updated at episode 2100\n","episode: 2200, steps: 4, epislon: 0.5166451184598713,win: False\n","Target network updated at episode 2200\n","episode: 2300, steps: 4, epislon: 0.5013736910009976,win: False\n","Target network updated at episode 2300\n","episode: 2400, steps: 1, epislon: 0.4865536691362134,win: False\n","Target network updated at episode 2400\n","episode: 2500, steps: 2, epislon: 0.4721717098423513,win: False\n","Target network updated at episode 2500\n","episode: 2600, steps: 1, epislon: 0.4582148645004558,win: False\n","Target network updated at episode 2600\n","episode: 2700, steps: 13, epislon: 0.4446705672376534,win: False\n","Target network updated at episode 2700\n","episode: 2800, steps: 6, epislon: 0.43152662361362526,win: False\n","Target network updated at episode 2800\n","episode: 2900, steps: 7, epislon: 0.4187711996414931,win: False\n","Target network updated at episode 2900\n","episode: 3000, steps: 4, epislon: 0.4063928111332365,win: False\n","Target network updated at episode 3000\n","episode: 3100, steps: 4, epislon: 0.39438031336004614,win: False\n","Target network updated at episode 3100\n","episode: 3200, steps: 1, epislon: 0.382722891018305,win: False\n","Target network updated at episode 3200\n","episode: 3300, steps: 3, epislon: 0.37141004849216325,win: False\n","Target network updated at episode 3300\n","episode: 3400, steps: 2, epislon: 0.36043160040394157,win: False\n","Target network updated at episode 3400\n","episode: 3500, steps: 1, epislon: 0.3497776624438522,win: False\n","Target network updated at episode 3500\n","episode: 3600, steps: 1, epislon: 0.3394386424707826,win: False\n","Target network updated at episode 3600\n","episode: 3700, steps: 1, epislon: 0.3294052318761297,win: False\n","Target network updated at episode 3700\n","episode: 3800, steps: 14, epislon: 0.31966839720291,win: False\n","Target network updated at episode 3800\n","episode: 3900, steps: 18, epislon: 0.31021937201259886,win: False\n","Target network updated at episode 3900\n","episode: 4000, steps: 7, epislon: 0.301049648992375,win: False\n","Target network updated at episode 4000\n","episode: 4100, steps: 3, epislon: 0.29215097229566794,win: False\n","Target network updated at episode 4100\n","episode: 4200, steps: 2, epislon: 0.2835153301091075,win: False\n","Target network updated at episode 4200\n","episode: 4300, steps: 20, epislon: 0.27513494743918765,win: False\n","Target network updated at episode 4300\n","episode: 4400, steps: 11, epislon: 0.26700227911214747,win: False\n","Target network updated at episode 4400\n","episode: 4500, steps: 2, epislon: 0.2591100029807669,win: False\n","Target network updated at episode 4500\n","episode: 4600, steps: 3, epislon: 0.25145101333196296,win: False\n","Target network updated at episode 4600\n","episode: 4700, steps: 17, epislon: 0.24401841448924763,win: False\n","Target network updated at episode 4700\n","episode: 4800, steps: 1, epislon: 0.2368055146042923,win: False\n","Target network updated at episode 4800\n","episode: 4900, steps: 4, epislon: 0.22980581963200428,win: False\n","Target network updated at episode 4900\n","episode: 5000, steps: 2, epislon: 0.2230130274836939,win: False\n","Target network updated at episode 5000\n","episode: 5100, steps: 5, epislon: 0.2164210223530665,win: False\n","Target network updated at episode 5100\n","episode: 5200, steps: 11, epislon: 0.21002386920993307,win: False\n","Target network updated at episode 5200\n","episode: 5300, steps: 18, epislon: 0.2038158084566782,win: False\n","Target network updated at episode 5300\n","episode: 5400, steps: 12, epislon: 0.19779125074267837,win: False\n","Target network updated at episode 5400\n","episode: 5500, steps: 20, epislon: 0.19194477193199883,win: False\n","Target network updated at episode 5500\n","episode: 5600, steps: 1, epislon: 0.18627110821984053,win: False\n","Target network updated at episode 5600\n","episode: 5700, steps: 7, epislon: 0.18076515139333815,win: False\n","Target network updated at episode 5700\n","episode: 5800, steps: 20, epislon: 0.1754219442324443,win: False\n","Target network updated at episode 5800\n","episode: 5900, steps: 16, epislon: 0.17023667604675766,win: False\n","Target network updated at episode 5900\n","episode: 6000, steps: 2, epislon: 0.1652046783442774,win: False\n","Target network updated at episode 6000\n","episode: 6100, steps: 8, epislon: 0.1603214206281839,win: False\n","Target network updated at episode 6100\n","episode: 6200, steps: 20, epislon: 0.1555825063178631,win: False\n","Target network updated at episode 6200\n","episode: 6300, steps: 20, epislon: 0.15098366879049854,win: False\n","Target network updated at episode 6300\n","episode: 6400, steps: 20, epislon: 0.1465207675396707,win: False\n","Target network updated at episode 6400\n","episode: 6500, steps: 4, epislon: 0.1421897844475032,win: False\n","Target network updated at episode 6500\n","episode: 6600, steps: 8, epislon: 0.13798682016700065,win: False\n","Target network updated at episode 6600\n","episode: 6700, steps: 20, epislon: 0.133908090611319,win: False\n","Target network updated at episode 6700\n","episode: 6800, steps: 10, epislon: 0.12994992354681023,win: False\n","Target network updated at episode 6800\n","episode: 6900, steps: 1, epislon: 0.12610875528677296,win: False\n","Target network updated at episode 6900\n","episode: 7000, steps: 4, epislon: 0.12238112748292994,win: False\n","Target network updated at episode 7000\n","episode: 7100, steps: 20, epislon: 0.118763684011748,win: False\n","Target network updated at episode 7100\n","episode: 7200, steps: 20, epislon: 0.11525316795279318,win: False\n","Target network updated at episode 7200\n","episode: 7300, steps: 19, epislon: 0.11184641865640337,win: False\n","Target network updated at episode 7300\n","episode: 7400, steps: 20, epislon: 0.10854036889803578,win: False\n","Target network updated at episode 7400\n","episode: 7500, steps: 8, epislon: 0.10533204211672993,win: False\n","Target network updated at episode 7500\n","episode: 7600, steps: 20, epislon: 0.10221854973519762,win: False\n","Target network updated at episode 7600\n","episode: 7700, steps: 3, epislon: 0.1,win: False\n","Target network updated at episode 7700\n","episode: 7800, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 7800\n","episode: 7900, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 7900\n","episode: 8000, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 8000\n","episode: 8100, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 8100\n","episode: 8200, steps: 6, epislon: 0.1,win: False\n","Target network updated at episode 8200\n","episode: 8300, steps: 1, epislon: 0.1,win: False\n","Target network updated at episode 8300\n","episode: 8400, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 8400\n","episode: 8500, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 8500\n","episode: 8600, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 8600\n","episode: 8700, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 8700\n","episode: 8800, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 8800\n","episode: 8900, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 8900\n","episode: 9000, steps: 2, epislon: 0.1,win: False\n","Target network updated at episode 9000\n","episode: 9100, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 9100\n","episode: 9200, steps: 20, epislon: 0.1,win: True\n","Target network updated at episode 9200\n","episode: 9300, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 9300\n","episode: 9400, steps: 19, epislon: 0.1,win: False\n","Target network updated at episode 9400\n","episode: 9500, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 9500\n","episode: 9600, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 9600\n","episode: 9700, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 9700\n","episode: 9800, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 9800\n","episode: 9900, steps: 20, epislon: 0.1,win: False\n","Target network updated at episode 9900\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from time import sleep\n","from IPython.display import clear_output\n","\n","# Set epsilon to 0 for full exploitation (greedy policy)\n","epsilon = 0\n","\n","win = False  # Track whether Pac-Man has won\n","\n","while not win:  # Keep running episodes until a win\n","    obs, info = env.reset()\n","    done = False\n","    truncated = False\n","    episode_reward = 0  # Track reward per episode\n","\n","    while not done and not truncated:\n","        env.render()\n","        obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n","\n","        # Choose action (greedy exploitation)\n","        with torch.no_grad():\n","            q_values = pred_network(obs)\n","            action = torch.argmax(q_values, dim=1).item()\n","\n","        # Take action\n","        obs, reward, done, truncated, info = env.step(action)\n","        episode_reward += reward  # Accumulate reward\n","\n","        sleep(0.5)  # Adjust for better visualization\n","        clear_output(wait=True)\n","\n","    env.render()\n","\n","    # Check if Pac-Man won\n","    if episode_reward >= 20:  # Adjust based on reward structure\n","        print(f\"Pac-Man won with a reward of {episode_reward}!\")\n","        win = True  # Exit loop\n","\n","env.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hJEkE1IHJrU","outputId":"bdf1703c-98c2-419b-effc-26b62f09f8c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["xxxxxx\n","x····x\n","x····x\n","x····x\n","x·ᗣᗧ◯x\n","xxxxxx\n","\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from time import sleep\n","from IPython.display import clear_output\n","\n","obs, info = env.reset()\n","done = False\n","truncated = False\n","\n","while not done and not truncated:\n","    env.render()\n","    obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n","\n","    t = np.random.random()\n","    if t > epsilon:  # Exploitation\n","        with torch.no_grad():\n","            q_values = pred_network(obs)  # Get Q-values from the network\n","            action = torch.argmax(q_values, dim=1).item()  # Select best action\n","    else:  # Exploration\n","        action = env.action_space.sample()  # Choose a random action\n","\n","    # Take action\n","    obs, reward, done, truncated, info = env.step(action)\n","\n","    sleep(1)  # Pause for visualization\n","    clear_output(wait=True)\n","\n","env.render()\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"QyGxhfIObaJl","executionInfo":{"status":"error","timestamp":1739516899307,"user_tz":480,"elapsed":3591,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"3bd0b8b9-4ad2-4048-f8bc-729029ab18eb"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'env' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a65fe7da5e62>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtruncated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from time import sleep\n","from IPython.display import clear_output\n","\n","obs, info = env.reset()\n","done = False\n","truncated = False\n","\n","while not done and not truncated:\n","    obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n","\n","    t = np.random.random()\n","    if t > epsilon:  # Exploitation\n","        with torch.no_grad():\n","            q_values = pred_network(obs)  # Get Q-values from the network\n","            action = torch.argmax(q_values, dim=1).item()  # Select best action\n","    else:  # Exploration\n","        action = env.action_space.sample()  # Choose a random action\n","\n","    # Take action\n","    obs, reward, done, truncated, info = env.step(action)\n","    obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Convert new observation\n","\n","\n","    sleep(1)  # Pause for visualization\n","    clear_output(wait=True)\n","\n","env.render()\n","env.close()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rDSZq8rBkYDx","executionInfo":{"status":"ok","timestamp":1739401824250,"user_tz":480,"elapsed":20310,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"d124135b-20e4-45d4-fb0c-b9b56d741289"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["xxxxxx\n","x····x\n","xᙉ···x\n","x··ᗧ·x\n","x····x\n","xxxxxx\n","\n"]}]},{"cell_type":"code","source":["obs, info = env.reset()\n","done = False\n","truncated = False\n","\n","while not done and not truncated:\n","    env.render()\n","    obs=torch.tensor(obs,dtype=torch.float32)\n","    action=#your code here\n","    obs, reward, done, truncated, info = env.step(action)\n","    sleep(1)\n","    clear_output(wait=True)\n","\n","env.render()\n","env.close()"],"metadata":{"id":"0SXyI97eNx6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class QNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear1 = nn.Linear(6 * 6, 128)  # Increase hidden units\n","        self.activation1 = nn.ReLU()\n","        self.linear2 = nn.Linear(128, 64)\n","        self.activation2 = nn.ReLU()\n","        self.linear3 = nn.Linear(64, 4)  # 4 actions\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.linear1(x)\n","        x = self.activation1(x)\n","        x = self.linear2(x)\n","        x = self.activation2(x)\n","        x = self.linear3(x)\n","        return x"],"metadata":{"id":"N8fIFyq7eeYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move model to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize networks on GPU\n","pred_network = QNetwork().to(device)\n","target_network = QNetwork().to(device)\n","secondary_target_network = QNetwork().to(device)  # If using double DQN\n","\n","# Optimizer (it automatically moves params to GPU)\n","Q_optimizer = torch.optim.Adam(pred_network.parameters(), lr=0.01)\n"],"metadata":{"id":"lN_YB11Vea21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\n","xxxxxx\n","x····x\n","xᗧ···x\n","x····x\n","x····x\n","xxxxxx\n","\n","\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"w4L6phsvR30R","executionInfo":{"status":"error","timestamp":1739516845005,"user_tz":480,"elapsed":200,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"1e13d937-df58-497c-ca46-a5fe234383b8"},"execution_count":2,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"unterminated string literal (detected at line 1) (<ipython-input-2-c723e25f4224>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-c723e25f4224>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"]}]},{"cell_type":"code","source":["print(\"\"\"\n","xxxxxx\n","x····x\n","xᗧ···x\n","x····x\n","x····x\n","xxxxxx\n","\"\"\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hRqKrwGpSNzL","executionInfo":{"status":"ok","timestamp":1739516863233,"user_tz":480,"elapsed":154,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"4a724adf-2a37-4db2-e9c4-f757a8abc6c4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","xxxxxx\n","x····x\n","xᗧ···x\n","x····x\n","x····x\n","xxxxxx\n","\n"]}]}]}