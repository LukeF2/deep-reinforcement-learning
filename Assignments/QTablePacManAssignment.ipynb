{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install gymnasium"],"metadata":{"id":"KJUp6QBRY-Qd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738878849104,"user_tz":480,"elapsed":12722,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"56462fc2-8b42-4490-9bbe-872c07fa4717"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"]}]},{"cell_type":"code","source":["from time import sleep\n","import numpy as np\n","from IPython.display import clear_output\n","import gymnasium as gym\n","from gymnasium.envs.registration import register"],"metadata":{"id":"K_GLg7lbayhm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Give colab access to your google drive:\n","from google.colab import drive\n","drive.mount('/gdrive')"],"metadata":{"id":"7PWN1PkGe66q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738878900745,"user_tz":480,"elapsed":18212,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"09f055e1-30bb-44cf-e5e3-ce7efbbe5f0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["%cd /gdrive/MyDrive/CSCI181V/"],"metadata":{"id":"1SCX1d90YjOg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738878902497,"user_tz":480,"elapsed":164,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"dc8219bf-0a77-4821-d4fd-68fb9a152d87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/CSCI181V\n"]}]},{"cell_type":"code","source":["#Import MiniPacMan environment class definition\n","from MiniPacManGym import MiniPacManEnv"],"metadata":{"id":"GCa5TYdVWL2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Register MiniPacMan in your gymnasium environments\n","register(\n","    id=\"MiniPacMan-v0\",\n","    entry_point=MiniPacManEnv,\n","    max_episode_steps=20\n",")"],"metadata":{"id":"TcY1Q97RRy6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create a MiniPacMan gymnasium environment\n","env = gym.make(\"MiniPacMan-v0\", render_mode=\"human\", frozen_ghost=True)"],"metadata":{"id":"k7hwnC7Ob9VJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class QNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(2, 64)\n","        self.fc2 = nn.Linear(64, 128)\n","        self.fc3 = nn.Linear(128, 64)\n","        self.fc4 = nn.Linear(64, 4)\n","\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","q_network = QNetwork().to(device)\n","opt = torch.optim.Adam(q_network.parameters(), lr=0.01)\n","loss_fn = nn.MSELoss()"],"metadata":{"id":"VWf-nhTYQd2K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gamma = 0.995\n","num_episodes = 10000\n","epsilon = 1.0\n","epsilon_min = 0.1\n","epsilon_decay = 0.9996\n","\n","for e in range(num_episodes):\n","    new_obs, info = env.reset()\n","    new_pos = np.argwhere(new_obs == 1)[0]\n","    done = False\n","    truncated = False\n","    steps = 0\n","\n","    while not done and not truncated:\n","        pos = new_pos\n","        state_tensor = torch.tensor([pos[0], pos[1]], dtype=torch.float32, device=device).unsqueeze(0)\n","        if np.random.random() > epsilon:\n","            with torch.no_grad(): # exploitation\n","                q_values = q_network(state_tensor)\n","                action = torch.argmax(q_values, dim=1).item()\n","        else:\n","\n","            action = np.random.randint(4) # exploration\n","\n","        new_obs, reward, done, truncated, info = env.step(action)\n","        steps += 1\n","        new_pos = np.argwhere(new_obs == 1)[0]\n","        next_state_tensor = torch.tensor([new_pos[0], new_pos[1]], dtype=torch.float32, device=device).unsqueeze(0) # update\n","        q_pred = q_network(state_tensor)[0, action]\n","\n","        with torch.no_grad():\n","            q_next = q_network(next_state_tensor)\n","            max_q_next = torch.max(q_next)\n","            q_target = reward + gamma * max_q_next\n","\n","        loss = loss_fn(q_pred, q_target)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n","\n","    if e % 100 == 0:\n","        print(f\"Episode: {e}, Steps: {steps}, Epsilon: {epsilon:.3f}, Win: {reward == 10}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"YYtolmR-TOYa","executionInfo":{"status":"error","timestamp":1738879758367,"user_tz":480,"elapsed":110,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"fc48a617-21f6-4f75-866f-ad4feffb34ee"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'optimizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-7e09df59c02d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class QNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(6 * 6, 128)\n","        self.activation = nn.ReLU()\n","        self.fc2 = nn.Linear(128, 4)\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.activation(x)\n","        x = self.fc2(x)  # Changed from self.linear2 to self.fc2\n","        return x\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","q_network = QNetwork().to(device)\n","opt = torch.optim.Adam(q_network.parameters(), lr=lr)\n","loss_fn = nn.MSELoss()\n","\n"],"metadata":{"id":"TUOoK0gTmxJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","gamma = 0.995\n","num_episodes = 10000\n","epsilon = 1.0\n","epsilon_min = 0.1\n","epsilon_decay = 0.9997  # Slower decay\n","\n","for e in range(num_episodes):\n","    new_obs, info = env.reset()\n","    done = False\n","    truncated = False\n","    steps = 0\n","\n","    while not done and not truncated:\n","        # Flatten full game state for the Q-network\n","        state_tensor = torch.tensor(new_obs.flatten(), dtype=torch.float32, device=device).unsqueeze(0)\n","\n","        # Choose action (Îµ-greedy)\n","        if np.random.random() > epsilon:\n","            with torch.no_grad():\n","                q_values = q_network(state_tensor)\n","                action = torch.argmax(q_values, dim=1).item()\n","        else:\n","            action = np.random.randint(4)\n","\n","        # Take action in the environment\n","        new_obs, reward, done, truncated, info = env.step(action)\n","        steps += 1\n","\n","        # Prepare next state\n","        next_state_tensor = torch.tensor(new_obs.flatten(), dtype=torch.float32, device=device).unsqueeze(0)\n","\n","        # Get Q-value prediction for the chosen action\n","        q_values = q_network(state_tensor)\n","        q_pred = q_values[0, action]\n","\n","        # Compute target Q-value\n","        with torch.no_grad():\n","            q_next = q_network(next_state_tensor)\n","            max_q_next = torch.max(q_next).detach()\n","            q_target = reward + gamma * max_q_next\n","\n","        # Compute loss\n","        loss = loss_fn(q_pred, q_target)\n","\n","        # Backpropagation\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","    # Update epsilon (slower decay)\n","    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n","\n","    # Progress monitoring\n","    if e % 100 == 0:\n","        print(f\"Episode: {e}, Steps: {steps}, Epsilon: {epsilon:.3f}, Win: {reward == 10}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPavwcOc-f9W","executionInfo":{"status":"ok","timestamp":1738823951387,"user_tz":480,"elapsed":122498,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"2299f252-1cc1-4048-b3dc-2c33248fc4d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 0, Steps: 12, Epsilon: 1.000, Win: True\n","Episode: 100, Steps: 1, Epsilon: 0.970, Win: False\n","Episode: 200, Steps: 3, Epsilon: 0.941, Win: False\n","Episode: 300, Steps: 1, Epsilon: 0.914, Win: False\n","Episode: 400, Steps: 5, Epsilon: 0.887, Win: False\n","Episode: 500, Steps: 8, Epsilon: 0.860, Win: False\n","Episode: 600, Steps: 4, Epsilon: 0.835, Win: False\n","Episode: 700, Steps: 1, Epsilon: 0.810, Win: False\n","Episode: 800, Steps: 1, Epsilon: 0.786, Win: False\n","Episode: 900, Steps: 4, Epsilon: 0.763, Win: False\n","Episode: 1000, Steps: 20, Epsilon: 0.741, Win: False\n","Episode: 1100, Steps: 3, Epsilon: 0.719, Win: False\n","Episode: 1200, Steps: 1, Epsilon: 0.697, Win: False\n","Episode: 1300, Steps: 1, Epsilon: 0.677, Win: False\n","Episode: 1400, Steps: 2, Epsilon: 0.657, Win: False\n","Episode: 1500, Steps: 1, Epsilon: 0.637, Win: False\n","Episode: 1600, Steps: 12, Epsilon: 0.619, Win: False\n","Episode: 1700, Steps: 2, Epsilon: 0.600, Win: False\n","Episode: 1800, Steps: 18, Epsilon: 0.583, Win: False\n","Episode: 1900, Steps: 1, Epsilon: 0.565, Win: False\n","Episode: 2000, Steps: 5, Epsilon: 0.549, Win: False\n","Episode: 2100, Steps: 6, Epsilon: 0.532, Win: False\n","Episode: 2200, Steps: 1, Epsilon: 0.517, Win: False\n","Episode: 2300, Steps: 1, Epsilon: 0.501, Win: False\n","Episode: 2400, Steps: 9, Epsilon: 0.487, Win: False\n","Episode: 2500, Steps: 4, Epsilon: 0.472, Win: False\n","Episode: 2600, Steps: 7, Epsilon: 0.458, Win: False\n","Episode: 2700, Steps: 1, Epsilon: 0.445, Win: False\n","Episode: 2800, Steps: 5, Epsilon: 0.432, Win: False\n","Episode: 2900, Steps: 3, Epsilon: 0.419, Win: False\n","Episode: 3000, Steps: 12, Epsilon: 0.406, Win: True\n","Episode: 3100, Steps: 1, Epsilon: 0.394, Win: False\n","Episode: 3200, Steps: 6, Epsilon: 0.383, Win: True\n","Episode: 3300, Steps: 6, Epsilon: 0.371, Win: True\n","Episode: 3400, Steps: 14, Epsilon: 0.360, Win: True\n","Episode: 3500, Steps: 3, Epsilon: 0.350, Win: False\n","Episode: 3600, Steps: 13, Epsilon: 0.339, Win: False\n","Episode: 3700, Steps: 1, Epsilon: 0.329, Win: False\n","Episode: 3800, Steps: 13, Epsilon: 0.320, Win: False\n","Episode: 3900, Steps: 3, Epsilon: 0.310, Win: False\n","Episode: 4000, Steps: 20, Epsilon: 0.301, Win: False\n","Episode: 4100, Steps: 17, Epsilon: 0.292, Win: False\n","Episode: 4200, Steps: 1, Epsilon: 0.284, Win: False\n","Episode: 4300, Steps: 9, Epsilon: 0.275, Win: False\n","Episode: 4400, Steps: 6, Epsilon: 0.267, Win: True\n","Episode: 4500, Steps: 4, Epsilon: 0.259, Win: False\n","Episode: 4600, Steps: 9, Epsilon: 0.251, Win: False\n","Episode: 4700, Steps: 1, Epsilon: 0.244, Win: False\n","Episode: 4800, Steps: 5, Epsilon: 0.237, Win: False\n","Episode: 4900, Steps: 8, Epsilon: 0.230, Win: True\n","Episode: 5000, Steps: 20, Epsilon: 0.223, Win: False\n","Episode: 5100, Steps: 20, Epsilon: 0.216, Win: False\n","Episode: 5200, Steps: 6, Epsilon: 0.210, Win: True\n","Episode: 5300, Steps: 4, Epsilon: 0.204, Win: False\n","Episode: 5400, Steps: 6, Epsilon: 0.198, Win: True\n","Episode: 5500, Steps: 12, Epsilon: 0.192, Win: True\n","Episode: 5600, Steps: 16, Epsilon: 0.186, Win: False\n","Episode: 5700, Steps: 14, Epsilon: 0.181, Win: True\n","Episode: 5800, Steps: 8, Epsilon: 0.175, Win: True\n","Episode: 5900, Steps: 6, Epsilon: 0.170, Win: True\n","Episode: 6000, Steps: 10, Epsilon: 0.165, Win: True\n","Episode: 6100, Steps: 6, Epsilon: 0.160, Win: True\n","Episode: 6200, Steps: 6, Epsilon: 0.156, Win: True\n","Episode: 6300, Steps: 20, Epsilon: 0.151, Win: False\n","Episode: 6400, Steps: 6, Epsilon: 0.147, Win: True\n","Episode: 6500, Steps: 1, Epsilon: 0.142, Win: False\n","Episode: 6600, Steps: 8, Epsilon: 0.138, Win: True\n","Episode: 6700, Steps: 8, Epsilon: 0.134, Win: True\n","Episode: 6800, Steps: 12, Epsilon: 0.130, Win: True\n","Episode: 6900, Steps: 20, Epsilon: 0.126, Win: False\n","Episode: 7000, Steps: 18, Epsilon: 0.122, Win: False\n","Episode: 7100, Steps: 6, Epsilon: 0.119, Win: True\n","Episode: 7200, Steps: 8, Epsilon: 0.115, Win: True\n","Episode: 7300, Steps: 6, Epsilon: 0.112, Win: True\n","Episode: 7400, Steps: 8, Epsilon: 0.109, Win: True\n","Episode: 7500, Steps: 6, Epsilon: 0.105, Win: True\n","Episode: 7600, Steps: 6, Epsilon: 0.102, Win: True\n","Episode: 7700, Steps: 5, Epsilon: 0.100, Win: False\n","Episode: 7800, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 7900, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 8000, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 8100, Steps: 18, Epsilon: 0.100, Win: True\n","Episode: 8200, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 8300, Steps: 9, Epsilon: 0.100, Win: False\n","Episode: 8400, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 8500, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 8600, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 8700, Steps: 2, Epsilon: 0.100, Win: False\n","Episode: 8800, Steps: 8, Epsilon: 0.100, Win: True\n","Episode: 8900, Steps: 20, Epsilon: 0.100, Win: True\n","Episode: 9000, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 9100, Steps: 20, Epsilon: 0.100, Win: False\n","Episode: 9200, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 9300, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 9400, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 9500, Steps: 10, Epsilon: 0.100, Win: True\n","Episode: 9600, Steps: 7, Epsilon: 0.100, Win: False\n","Episode: 9700, Steps: 6, Epsilon: 0.100, Win: True\n","Episode: 9800, Steps: 16, Epsilon: 0.100, Win: False\n","Episode: 9900, Steps: 8, Epsilon: 0.100, Win: True\n"]}]},{"cell_type":"code","source":["obs, info = env.reset()\n","done = False\n","truncated = False\n","\n","while not done and not truncated:\n","    env.render()\n","\n","    # Convert full 6x6 board to a flattened tensor\n","    state_tensor = torch.tensor(obs.flatten(), dtype=torch.float32, device=device).unsqueeze(0)\n","\n","    # Get the Q-values for the current state from Q-network\n","    with torch.no_grad():\n","        q_values = q_network(state_tensor)\n","\n","    # Choose the action with the highest Q-value\n","    action = torch.argmax(q_values, dim=1).item()\n","\n","    # Take action in environment\n","    obs, reward, done, truncated, info = env.step(action)\n","\n","    sleep(1)\n","    clear_output(wait=True)\n","\n","env.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0dP9METAx-C","executionInfo":{"status":"ok","timestamp":1738824433381,"user_tz":480,"elapsed":6245,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"e6bac40e-8a90-48a5-e167-0262104759ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["xxxxxx\n","xÂ·Â·Â·Â·x\n","xÂ·Â·á£Â·x\n","xÂ·Â·Â·á§x\n","xÂ·Â·Â·â¯x\n","xxxxxx\n","\n"]}]},{"cell_type":"code","source":["gamma = 0.995\n","lr = 0.002\n","num_episodes = 10000\n","epsilon = 1.0\n","epsilon_min = 0.1\n","epsilon_decay = 0.9996\n","\n","for e in range(num_episodes):\n","    new_obs, info = env.reset()\n","    new_pos = np.argwhere(new_obs == 1)[0]\n","    done = False\n","    truncated = False\n","    steps = 0\n","\n","    while not done and not truncated:\n","        pos = new_pos\n","        state_tensor = torch.tensor(new_obs.flatten(), dtype=torch.float32, device=device).unsqueeze(0)\n","        next_state_tensor = torch.tensor(new_obs.flatten(), dtype=torch.float32, device=device).unsqueeze(0)\n","        if np.random.random() > epsilon:\n","            with torch.no_grad(): # exploitation\n","                q_values = q_network(state_tensor)\n","                action = torch.argmax(q_values, dim=1).item()\n","        else:\n","\n","            action = np.random.randint(4) # exploration\n","\n","        new_obs, reward, done, truncated, info = env.step(action)\n","        steps += 1\n","        new_pos = np.argwhere(new_obs == 1)[0]\n","        next_state_tensor = torch.tensor([new_pos[0], new_pos[1]], dtype=torch.float32, device=device).unsqueeze(0) # update\n","        q_pred = q_network(state_tensor)[0, action]\n","\n","        with torch.no_grad():\n","            q_next = q_network(next_state_tensor)\n","            max_q_next = torch.max(q_next).detach()\n","            q_target = reward + gamma * max_q_next\n","\n","        loss = loss_fn(q_pred, q_target)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epsilon=max(0.01, epsilon - 1.0/num_episodes)\n","\n","    if e % 100 == 0:\n","        print(f\"Episode: {e}, Steps: {steps}, Epsilon: {epsilon:.3f}, Win: {reward == 10}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"mNjtP6Vgob6r","executionInfo":{"status":"error","timestamp":1738823811324,"user_tz":480,"elapsed":139,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"b0fb7867-8b36-4f98-bb62-8400bf7c8301"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x2 and 36x128)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-105-e952486663af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mq_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mmax_q_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mq_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_q_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-103-e18af0eca382>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Changed from self.linear2 to self.fc2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 36x128)"]}]},{"cell_type":"code","source":["#Run this code cell to see your trained agent in action!\n","\n","obs, info = env.reset()\n","done = False\n","truncated = False\n","\n","while not done and not truncated:\n","    env.render()\n","    pos=pos = np.argwhere(obs == 1)[0]  # pacman position\n","    # obs == 1 creates boolean array from obs where each element = 1 if True and 0 is false, in this instance 1 represents Pacman's position\n","    # np.argwhere returns indices of all elements in array that are True, so it returns the indices of pacman's position\n","    # [0] picks the first (and only) coordinate from the result, so pos becomes [1,1] if that's where pacman is located\n","    state_tensor = torch.tensor([pos[0], pos[1]], dtype=torch.float32, device=device).unsqueeze(0) # convert position to tensor\n","    # Get the Q-values for the current state from Q-network.\n","    with torch.no_grad():\n","        q_values = q_network(state_tensor)\n","    action = torch.argmax(q_values, dim=1).item()     # Choose the action with the highest Q-value.\n","    obs, reward, done, truncated, info = env.step(action)\n","    sleep(1)\n","    clear_output(wait=True)\n","\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"1oaaD41E2GR6","executionInfo":{"status":"error","timestamp":1738824070193,"user_tz":480,"elapsed":161,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"c3599f45-e28a-4100-a3d0-dd3988cb8e1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["xxxxxx\n","xá§Â·Â·Â·x\n","xÂ·Â·á£Â·x\n","xÂ·Â·Â·Â·x\n","xÂ·Â·Â·â¯x\n","xxxxxx\n","\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x2 and 36x128)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-107-e8f81523819e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Get the Q-values for the current state from Q-network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Choose the action with the highest Q-value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-103-e18af0eca382>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Changed from self.linear2 to self.fc2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 36x128)"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from time import sleep\n","from IPython.display import clear_output\n","\n","# Keep running until Pac-Man succeeds\n","success = False\n","attempts = 0  # Track the number of attempts\n","\n","while not success:\n","    attempts += 1\n","    print(f\"Attempt: {attempts}\")\n","\n","    obs, info = env.reset()\n","    done = False\n","    truncated = False\n","    total_reward = 0  # Track total reward\n","\n","    while not done and not truncated:\n","        env.render()\n","        pos = np.argwhere(obs == 1)[0]  # Get Pac-Man's position\n","\n","        # Convert position to tensor\n","        state_tensor = torch.tensor([pos[0], pos[1]], dtype=torch.float32, device=device).unsqueeze(0)\n","\n","        # Get Q-values from the Q-network\n","        with torch.no_grad():\n","            q_values = q_network(state_tensor)\n","\n","        # Choose the best action\n","        action = torch.argmax(q_values, dim=1).item()\n","\n","        # Take the step\n","        obs, reward, done, truncated, info = env.step(action)\n","        total_reward += reward  # Accumulate total reward\n","\n","        sleep(0.5)  # Adjust sleep time for visibility\n","        clear_output(wait=True)\n","\n","    # Check if Pac-Man won\n","    if total_reward >= 10:\n","        success = True\n","        print(f\"â Success! Pac-Man won after {attempts} attempts.\")\n","    else:\n","        print(\"â Failed. Restarting...\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"iXVe3deRyoIz","executionInfo":{"status":"error","timestamp":1738823302130,"user_tz":480,"elapsed":4204,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"11e6fad0-af61-489c-9401-f7a77ec3a8d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["xxxxxx\n","xá§Â·Â·Â·x\n","xÂ·Â·á£Â·x\n","xÂ·Â·Â·Â·x\n","xÂ·Â·Â·â¯x\n","xxxxxx\n","\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-8d7a3d4a1dad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m  \u001b[0;31m# Accumulate total reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust sleep time for visibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#Run this code cell to see your trained agent in action!\n","\n","obs, info = env.reset()\n","done = False\n","truncated = False\n","\n","while not done and not truncated:\n","    env.render()\n","    pos=pos = np.argwhere(obs == 1)[0]  # pacman position\n","    # obs == 1 creates boolean array from obs where each element = 1 if True and 0 is false, in this instance 1 represents Pacman's position\n","    # np.argwhere returns indices of all elements in array that are True, so it returns the indices of pacman's position\n","    # [0] picks the first (and only) coordinate from the result, so pos becomes [1,1] if that's where pacman is located\n","    state_tensor = torch.tensor([pos[0], pos[1]], dtype=torch.float32, device=device).unsqueeze(0) # convert position to tensor\n","    # Get the Q-values for the current state from your Q-network.\n","    with torch.no_grad():\n","        q_values = q_network(state_tensor)\n","    action = torch.argmax(q_values, dim=1).item()     # Choose the action with the highest Q-value.\n","    obs, reward, done, truncated, info = env.step(action)\n","    sleep(1)\n","    clear_output(wait=True)\n","\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaBI6QVItXYZ","executionInfo":{"status":"ok","timestamp":1738821719780,"user_tz":480,"elapsed":4184,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"11e33534-0b59-4aea-c21f-00b43081e9e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["xxxxxx\n","xÂ·Â·Â·á§x\n","xÂ·Â·á£Â·x\n","xÂ·Â·Â·Â·x\n","xÂ·Â·Â·â¯x\n","xxxxxx\n","\n"]}]},{"cell_type":"code","source":["#set hyperparams -- feel free to play with these!\n","gamma=0.95\n","alpha=0.9\n","num_episodes=10000\n","\n","#initialize epsilon, Q\n","epsilon=1\n","Q=np.zeros((6,6,4)) #First two coordinates encode state, last encodes action\n","\n","for e in range(num_episodes):\n","  new_obs,info=env.reset()\n","  new_pos=np.argwhere(new_obs==1)[0] #current pacman position\n","  done=False\n","  truncated=False\n","  steps=0\n","\n","  while not done and not truncated: #Loop for one episode\n","    obs=new_obs\n","    pos=new_pos\n","\n","    #choose action, essentially here, exploration uses randint to choose one of 4 random actions, exploitation used np.argmax to choose action that expects best reward\n","    t=np.random.random()\n","    if t>epsilon: # t is a random number, epsilon is the probability threshold for exploring, at the start, epsilon is set high meaning high chance of exploration\n","      # over time, epsilon decays leading to more exploitation over time\n","      # so if t < epsilon, we choose a random action, if t > epsilon we exploit choosing action with most ideal reward\n","      action=action = np.argmax(Q[pos[0], pos[1], :]) #exploitation\n","      # Q is your Q-table where the first two indices represent the state\n","      # and the last index represents the action.\n","      # np.argmax(Q[pos[0], pos[1], :]) returns the index of the action with the highest Q-value for that state.\n","      # This is the action you believe will yield the best long-term reward.\n","    else:\n","      action=action = np.random.randint(4) #exploration\n","      # The environment has 4 possible actions (up, down, left, right), so np.random.randint(4) returns a random integer between 0 and 3.\n","\n","    # we establish this balance of exploration and exploitation with epsilon because exploration is needed to gather more info about environment\n","    # such that exploitation will actually work, since exploitation uses current best known info to make optimal decision\n","\n","    #take a step:\n","    new_obs,reward, done, truncated, info=env.step(action)\n","    steps+=1\n","    new_pos=np.argwhere(new_obs==1)[0] #next pacman position\n","\n","    #Q-table update rule:\n","    Q[pos[0],pos[1],action]=Q[pos[0], pos[1], action] = Q[pos[0], pos[1], action] + alpha * (reward + gamma * np.max(Q[new_pos[0], new_pos[1], :]) - Q[pos[0], pos[1], action])\n","    # pos = current state, action = action, update the Q-value at the current state (pos) for the action taken.\n","    # reward is immediate reward after taking action, np.max finds max q-value for next state (best expected reward)\n","    # alpha is learning rate, gamma is discount factor, in this case 0.999 which determines importance of future reward\n","    # gamma used to disincentivize processes that take too many steps\n","\n","  #reduce episilon if its not too low\n","  #Should be close to zero after 50 - 60% of episodes, and then level off\n","  epsilon=epsilon = max(0.1, epsilon * 0.995)\n","  # we want to start with exploration then shift to exploitation\n","  # .995 gradually reduces epsilon each episode\n","  # max(0.1) ensures epsilon never goes below 0.1, this means there will always be a 10% chance to explore, avoid local optima\n","\n","  #periodic reporting:\n","  if e%100==0:\n","    print(f'episode: {e}, steps: {steps}, epislon: {epsilon}, win: {reward==10}')\n"],"metadata":{"id":"0fe-YvvwKpAZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738692207306,"user_tz":480,"elapsed":6382,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"641147e4-4734-4135-8675-dea149101b08","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["episode: 0, steps: 8, epislon: 0.995, win: False\n","episode: 100, steps: 5, epislon: 0.6027415843082742, win: False\n","episode: 200, steps: 4, epislon: 0.36512303261753626, win: False\n","episode: 300, steps: 8, epislon: 0.2211807388415433, win: True\n","episode: 400, steps: 1, epislon: 0.13398475271138335, win: False\n","episode: 500, steps: 10, epislon: 0.1, win: True\n","episode: 600, steps: 6, epislon: 0.1, win: True\n","episode: 700, steps: 6, epislon: 0.1, win: True\n","episode: 800, steps: 6, epislon: 0.1, win: True\n","episode: 900, steps: 6, epislon: 0.1, win: True\n","episode: 1000, steps: 6, epislon: 0.1, win: True\n","episode: 1100, steps: 9, epislon: 0.1, win: False\n","episode: 1200, steps: 6, epislon: 0.1, win: True\n","episode: 1300, steps: 1, epislon: 0.1, win: False\n","episode: 1400, steps: 5, epislon: 0.1, win: False\n","episode: 1500, steps: 6, epislon: 0.1, win: True\n","episode: 1600, steps: 14, epislon: 0.1, win: True\n","episode: 1700, steps: 6, epislon: 0.1, win: True\n","episode: 1800, steps: 7, epislon: 0.1, win: False\n","episode: 1900, steps: 6, epislon: 0.1, win: True\n","episode: 2000, steps: 1, epislon: 0.1, win: False\n","episode: 2100, steps: 6, epislon: 0.1, win: True\n","episode: 2200, steps: 1, epislon: 0.1, win: False\n","episode: 2300, steps: 6, epislon: 0.1, win: True\n","episode: 2400, steps: 6, epislon: 0.1, win: True\n","episode: 2500, steps: 6, epislon: 0.1, win: True\n","episode: 2600, steps: 8, epislon: 0.1, win: False\n","episode: 2700, steps: 6, epislon: 0.1, win: True\n","episode: 2800, steps: 6, epislon: 0.1, win: True\n","episode: 2900, steps: 6, epislon: 0.1, win: True\n","episode: 3000, steps: 6, epislon: 0.1, win: True\n","episode: 3100, steps: 2, epislon: 0.1, win: False\n","episode: 3200, steps: 1, epislon: 0.1, win: False\n","episode: 3300, steps: 6, epislon: 0.1, win: True\n","episode: 3400, steps: 6, epislon: 0.1, win: True\n","episode: 3500, steps: 8, epislon: 0.1, win: True\n","episode: 3600, steps: 6, epislon: 0.1, win: True\n","episode: 3700, steps: 6, epislon: 0.1, win: True\n","episode: 3800, steps: 6, epislon: 0.1, win: True\n","episode: 3900, steps: 1, epislon: 0.1, win: False\n","episode: 4000, steps: 6, epislon: 0.1, win: True\n","episode: 4100, steps: 1, epislon: 0.1, win: False\n","episode: 4200, steps: 8, epislon: 0.1, win: True\n","episode: 4300, steps: 6, epislon: 0.1, win: True\n","episode: 4400, steps: 8, epislon: 0.1, win: True\n","episode: 4500, steps: 6, epislon: 0.1, win: True\n","episode: 4600, steps: 6, epislon: 0.1, win: True\n","episode: 4700, steps: 6, epislon: 0.1, win: True\n","episode: 4800, steps: 6, epislon: 0.1, win: True\n","episode: 4900, steps: 8, epislon: 0.1, win: True\n","episode: 5000, steps: 8, epislon: 0.1, win: False\n","episode: 5100, steps: 10, epislon: 0.1, win: True\n","episode: 5200, steps: 6, epislon: 0.1, win: True\n","episode: 5300, steps: 6, epislon: 0.1, win: True\n","episode: 5400, steps: 8, epislon: 0.1, win: True\n","episode: 5500, steps: 6, epislon: 0.1, win: True\n","episode: 5600, steps: 6, epislon: 0.1, win: True\n","episode: 5700, steps: 6, epislon: 0.1, win: True\n","episode: 5800, steps: 1, epislon: 0.1, win: False\n","episode: 5900, steps: 6, epislon: 0.1, win: True\n","episode: 6000, steps: 6, epislon: 0.1, win: True\n","episode: 6100, steps: 6, epislon: 0.1, win: True\n","episode: 6200, steps: 6, epislon: 0.1, win: True\n","episode: 6300, steps: 6, epislon: 0.1, win: True\n","episode: 6400, steps: 6, epislon: 0.1, win: True\n","episode: 6500, steps: 10, epislon: 0.1, win: True\n","episode: 6600, steps: 6, epislon: 0.1, win: True\n","episode: 6700, steps: 6, epislon: 0.1, win: True\n","episode: 6800, steps: 8, epislon: 0.1, win: True\n","episode: 6900, steps: 6, epislon: 0.1, win: True\n","episode: 7000, steps: 6, epislon: 0.1, win: True\n","episode: 7100, steps: 6, epislon: 0.1, win: True\n","episode: 7200, steps: 6, epislon: 0.1, win: True\n","episode: 7300, steps: 6, epislon: 0.1, win: True\n","episode: 7400, steps: 6, epislon: 0.1, win: True\n","episode: 7500, steps: 4, epislon: 0.1, win: False\n","episode: 7600, steps: 6, epislon: 0.1, win: True\n","episode: 7700, steps: 6, epislon: 0.1, win: True\n","episode: 7800, steps: 6, epislon: 0.1, win: True\n","episode: 7900, steps: 4, epislon: 0.1, win: False\n","episode: 8000, steps: 10, epislon: 0.1, win: True\n","episode: 8100, steps: 4, epislon: 0.1, win: False\n","episode: 8200, steps: 6, epislon: 0.1, win: True\n","episode: 8300, steps: 8, epislon: 0.1, win: True\n","episode: 8400, steps: 6, epislon: 0.1, win: True\n","episode: 8500, steps: 6, epislon: 0.1, win: True\n","episode: 8600, steps: 10, epislon: 0.1, win: True\n","episode: 8700, steps: 6, epislon: 0.1, win: True\n","episode: 8800, steps: 6, epislon: 0.1, win: True\n","episode: 8900, steps: 4, epislon: 0.1, win: False\n","episode: 9000, steps: 6, epislon: 0.1, win: True\n","episode: 9100, steps: 1, epislon: 0.1, win: False\n","episode: 9200, steps: 6, epislon: 0.1, win: True\n","episode: 9300, steps: 6, epislon: 0.1, win: True\n","episode: 9400, steps: 6, epislon: 0.1, win: True\n","episode: 9500, steps: 4, epislon: 0.1, win: False\n","episode: 9600, steps: 6, epislon: 0.1, win: True\n","episode: 9700, steps: 6, epislon: 0.1, win: True\n","episode: 9800, steps: 6, epislon: 0.1, win: True\n","episode: 9900, steps: 10, epislon: 0.1, win: True\n"]}]},{"cell_type":"code","source":["#Run this code cell to see your trained agent in action!\n","\n","obs, info = env.reset()\n","done = False\n","truncated = False\n","\n","while not done and not truncated:\n","    env.render()\n","    pos=pos = np.argwhere(obs == 1)[0]  # pacman position\n","    # obs == 1 creates boolean array from obs where each element = 1 if True and 0 is false, in this instance 1 represents Pacman's position\n","    # np.argwhere returns indices of all elements in array that are True, so it returns the indices of pacman's position\n","    # [0] picks the first (and only) coordinate from the result, so pos becomes [1,1] if that's where pacman is located\n","    action = np.argmax(Q[pos[0], pos[1], :]) # pick the action with the highest Q-value for the current state.\n","    # our q-table is a 3d np array, first two indices represent the state, (pacman's pos on grid) and last represents 4 possible actions\n","    # by indexing (Q[pos...]) we are retrieving Q-values for all actions at state given by Pacman's current position\n","    # np.argmax selects the action that gives us the greatest Q value among the 4 possible actions\n","    obs, reward, done, truncated, info = env.step(action)\n","    sleep(1)\n","    clear_output(wait=True)\n","\n","env.close()"],"metadata":{"id":"cHj_fgzAtK2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run this code cell to see your trained agent in action!\n","\n","obs, info = env.reset()\n","done = False\n","truncated = False\n","\n","while not done and not truncated:\n","    env.render()\n","    pos=pos = np.argwhere(obs == 1)[0]  # pacman position\n","    # obs == 1 creates boolean array from obs where each element = 1 if True and 0 is false, in this instance 1 represents Pacman's position\n","    # np.argwhere returns indices of all elements in array that are True, so it returns the indices of pacman's position\n","    # [0] picks the first (and only) coordinate from the result, so pos becomes [1,1] if that's where pacman is located\n","    action = np.argmax(Q[pos[0], pos[1], :]) # pick the action with the highest Q-value for the current state.\n","    # our q-table is a 3d np array, first two indices represent the state, (pacman's pos on grid) and last represents 4 possible actions\n","    # by indexing (Q[pos...]) we are retrieving Q-values for all actions at state given by Pacman's current position\n","    # np.argmax selects the action that gives us the greatest Q value among the 4 possible actions\n","    obs, reward, done, truncated, info = env.step(action)\n","    sleep(1)\n","    clear_output(wait=True)\n","\n","env.close()"],"metadata":{"id":"0SXyI97eNx6L","colab":{"base_uri":"https://localhost:8080/","height":341},"executionInfo":{"status":"error","timestamp":1738819265740,"user_tz":480,"elapsed":44,"user":{"displayName":"Luke Feng","userId":"05616786307952523561"}},"outputId":"c387856f-ebb6-4a48-c325-30a6c8929242"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["xxxxxx\n","xá§Â·Â·Â·x\n","xÂ·Â·á£Â·x\n","xÂ·Â·Â·Â·x\n","xÂ·Â·Â·â¯x\n","xxxxxx\n","\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'Q' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-223945b9beb4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# np.argwhere returns indices of all elements in array that are True, so it returns the indices of pacman's position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# [0] picks the first (and only) coordinate from the result, so pos becomes [1,1] if that's where pacman is located\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pick the action with the highest Q-value for the current state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# our q-table is a 3d np array, first two indices represent the state, (pacman's pos on grid) and last represents 4 possible actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# by indexing (Q[pos...]) we are retrieving Q-values for all actions at state given by Pacman's current position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Q' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VZjO1S_URYs2"},"execution_count":null,"outputs":[]}]}